#!/usr/bin/env python3
"""
Web Scraper Principal
Este script l√™ o arquivo JSON de configura√ß√£o e realiza web scraping das p√°ginas especificadas.
"""

import json
import requests
import sqlite3
import time
import sys
import re
from pathlib import Path
from bs4 import BeautifulSoup
from datetime import datetime


class WebScraper:
    def __init__(self, config_file="data/sites.json"):
        """
        Inicializa o WebScraper com o arquivo de configura√ß√£o.
        
        Args:
            config_file (str): Caminho para o arquivo JSON de configura√ß√£o
        """
        self.config_file = config_file
        self.sites = []
        self.session = requests.Session()
        
    def load_config(self):
        """Carrega a configura√ß√£o dos sites do arquivo JSON."""
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
                self.sites = config.get('sites', [])
                print(f"‚úÖ Configura√ß√£o carregada: {len(self.sites)} sites encontrados")
        except FileNotFoundError:
            print(f"‚ùå Erro: Arquivo de configura√ß√£o '{self.config_file}' n√£o encontrado")
            sys.exit(1)
        except json.JSONDecodeError as e:
            print(f"‚ùå Erro ao decodificar JSON: {e}")
            sys.exit(1)
    
    def scrape_site(self, site_config):
        """
        Realiza o scraping de um site espec√≠fico.
        
        Args:
            site_config (dict): Configura√ß√£o do site a ser processado
            
        Returns:
            dict: Dados extra√≠dos do site
        """
        name = site_config.get('name', 'Site Desconhecido')
        url = site_config.get('url')
        selectors = site_config.get('selectors', {})
        headers = site_config.get('headers', {})
        
        if not url:
            print(f"‚ö†Ô∏è  Site '{name}': URL n√£o encontrada")
            return None
            
        print(f"\nüîç Fazendo scraping de: {name}")
        print(f"   URL: {url}")
        
        try:
            # Fazer a requisi√ß√£o HTTP
            response = self.session.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            # Parse do HTML
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extrair dados baseado nos seletores
            extracted_data = {
                'site_name': name,
                'url': url,
                'scraped_at': datetime.now().isoformat(),
                'data': {}
            }
            
            for field, selector in selectors.items():
                elements = soup.select(selector)
                if elements:
                    # Se h√° m√∫ltiplos elementos, pegar o texto de todos
                    if len(elements) == 1:
                        extracted_data['data'][field] = elements[0].get_text(strip=True)
                    else:
                        extracted_data['data'][field] = [elem.get_text(strip=True) for elem in elements]
                else:
                    extracted_data['data'][field] = None
                    print(f"   ‚ö†Ô∏è  Seletor '{selector}' para '{field}' n√£o encontrou elementos")
            
            # Extra√ß√£o abrangente de pre√ßos
            print(f"   üîç Extraindo pre√ßos de m√∫ltiplas fontes...")
            
            # Primeira extra√ß√£o imediata
            prices_found = self.extract_price_comprehensive(soup, url)
            
            # Se n√£o encontrou pre√ßos satisfat√≥rios, aguardar e tentar novamente
            if not prices_found or (len(prices_found) == 1 and 'json_structured' in prices_found):
                print(f"   ‚è≥ Aguardando 3 segundos para carregamento din√¢mico...")
                time.sleep(3)
                
                # Fazer nova requisi√ß√£o para pegar conte√∫do atualizado
                try:
                    response2 = self.session.get(url, headers=headers, timeout=30)
                    response2.raise_for_status()
                    soup2 = BeautifulSoup(response2.content, 'html.parser')
                    prices_found2 = self.extract_price_comprehensive(soup2, url)
                    
                    # Combinar resultados, priorizando os mais recentes
                    prices_found.update(prices_found2)
                    
                except Exception:
                    print(f"   ‚ö†Ô∏è  Segunda tentativa falhou, usando dados iniciais")
            
            # Determinar o melhor pre√ßo
            best_price = None
            price_source = None
            
            if prices_found:
                # Prioridade: NextJS data > Script data > JSON structured > Text content
                if 'nextjs_data' in prices_found:
                    best_price = prices_found['nextjs_data']
                    price_source = "NextJS"
                elif 'script_data' in prices_found and isinstance(prices_found['script_data'], list):
                    # Pegar o pre√ßo mais comum ou o √∫ltimo
                    script_prices = prices_found['script_data']
                    best_price = script_prices[-1]  # √öltimo encontrado
                    price_source = "Script"
                elif 'json_structured' in prices_found:
                    best_price = prices_found['json_structured']
                    price_source = "JSON"
                elif 'text_content' in prices_found and isinstance(prices_found['text_content'], list):
                    text_prices = prices_found['text_content']
                    best_price = text_prices[-1]  # √öltimo encontrado
                    price_source = "Texto"
            
            if best_price:
                extracted_data['data']['price_final'] = f"R$ {best_price:.2f}"
                extracted_data['data']['price_source'] = price_source
                print(f"   ‚úÖ Melhor pre√ßo encontrado: R$ {best_price:.2f} (fonte: {price_source})")
                
                # Adicionar todos os pre√ßos encontrados para debug
                extracted_data['data']['all_prices_debug'] = {
                    k: (f"R$ {v:.2f}" if isinstance(v, (int, float)) else 
                        [f"R$ {p:.2f}" for p in v] if isinstance(v, list) else str(v))
                    for k, v in prices_found.items()
                }
            else:
                print(f"   ‚ö†Ô∏è  Nenhum pre√ßo v√°lido encontrado")
                extracted_data['data']['price_final'] = "N√£o encontrado"
            
            print(f"   ‚úÖ Scraping conclu√≠do com sucesso!")
            return extracted_data
            
        except requests.exceptions.RequestException as e:
            print(f"   ‚ùå Erro na requisi√ß√£o: {e}")
            return None
        except Exception as e:
            print(f"   ‚ùå Erro durante o scraping: {e}")
            return None
    
    def display_results(self, results):
        """
        Exibe os resultados do scraping no console de forma formatada.
        
        Args:
            results (list): Lista com os dados extra√≠dos
        """
        print("\n" + "="*80)
        print("üìä RESULTADOS DO WEB SCRAPING")
        print("="*80)
        
        if not results:
            print("‚ùå Nenhum resultado encontrado.")
            return
        
        for i, result in enumerate(results, 1):
            if result is None:
                continue
                
            print(f"\nüè∑Ô∏è  SITE {i}: {result['site_name']}")
            print(f"üîó URL: {result['url']}")
            print(f"üìÖ Extra√≠do em: {result['scraped_at']}")
            print("-" * 60)
            
            data = result.get('data', {})
            if not data:
                print("   ‚ö†Ô∏è  Nenhum dado extra√≠do")
                continue
                
            for field, value in data.items():
                print(f"üìã {field.upper()}:")
                if isinstance(value, list):
                    for item in value:
                        print(f"   ‚Ä¢ {item}")
                elif value:
                    print(f"   {value}")
                else:
                    print(f"   (n√£o encontrado)")
                print()
    
    def run(self):
        """Executa o processo completo de scraping."""
        print("üöÄ Iniciando Web Scraper")
        print("-" * 40)
        
        # Carregar configura√ß√£o
        self.load_config()
        
        if not self.sites:
            print("‚ùå Nenhum site configurado para scraping")
            return
        
        # Filtrar apenas sites habilitados
        enabled_sites = [site for site in self.sites if site.get('enabled', True)]
        
        if not enabled_sites:
            print("‚ùå Nenhum site habilitado para scraping")
            return
            
        print(f"üéØ Processando {len(enabled_sites)} site(s) habilitado(s)...")
        
        # Fazer scraping de cada site
        results = []
        for site in enabled_sites:
            result = self.scrape_site(site)
            results.append(result)
            
            # Pausa entre requisi√ß√µes para ser educado com os servidores
            time.sleep(1)
        
        # Exibir resultados
        self.display_results(results)
        
        print(f"\nüéâ Scraping finalizado! Processados {len([r for r in results if r])} site(s) com sucesso.")

    def extract_price_comprehensive(self, soup, url):
        """
        Extrai pre√ßo de m√∫ltiplas fontes para pegar o valor mais atualizado.
        
        Args:
            soup: BeautifulSoup object
            url: URL da p√°gina
            
        Returns:
            dict: Dicion√°rio com diferentes pre√ßos encontrados
        """
        prices = {}
        
        # 1. Extrair do JSON estruturado (pode ser o pre√ßo inicial)
        try:
            json_scripts = soup.select('script[type="application/ld+json"]')
            for script in json_scripts:
                script_content = script.get_text()
                if '"offers"' in script_content and '"price"' in script_content:
                    try:
                        data = json.loads(script_content)
                        if isinstance(data, dict) and 'offers' in data:
                            if isinstance(data['offers'], dict) and 'price' in data['offers']:
                                prices['json_structured'] = data['offers']['price']
                            elif isinstance(data['offers'], list) and len(data['offers']) > 0:
                                offer = data['offers'][0]
                                if 'price' in offer:
                                    prices['json_structured'] = offer['price']
                    except json.JSONDecodeError:
                        continue
        except Exception:
            pass
        
        # 2. Buscar pre√ßos no JavaScript/Next.js data
        try:
            next_data_scripts = soup.select('script#__NEXT_DATA__')
            for script in next_data_scripts:
                script_content = script.get_text()
                # Procurar por padr√µes de pre√ßo no JavaScript
                price_patterns = [
                    r'"price":\s*(\d+\.?\d*)',
                    r'"lowPrice":\s*(\d+\.?\d*)',
                    r'"value":\s*(\d+\.?\d*)',
                    r'"Price":\s*(\d+\.?\d*)'
                ]
                
                for pattern in price_patterns:
                    matches = re.findall(pattern, script_content)
                    if matches:
                        # Pegar o √∫ltimo pre√ßo encontrado (geralmente o mais atualizado)
                        latest_price = matches[-1]
                        prices['nextjs_data'] = float(latest_price)
                        break
        except Exception:
            pass
        
        # 3. Buscar pre√ßos em todos os scripts que contenham dados de pre√ßo
        try:
            all_scripts = soup.find_all('script')
            for script in all_scripts:
                if script.string:
                    content = script.string
                    # Procurar por diferentes padr√µes de pre√ßo
                    patterns = [
                        r'"price":\s*(\d+\.?\d*)',
                        r'"valor":\s*(\d+\.?\d*)',
                        r'"preco":\s*(\d+\.?\d*)',
                        r'price:\s*(\d+\.?\d*)',
                        r'valor:\s*(\d+\.?\d*)'
                    ]
                    
                    for pattern in patterns:
                        matches = re.findall(pattern, content)
                        if matches:
                            # Converter todos os pre√ßos encontrados
                            script_prices = [float(price) for price in matches if float(price) > 0]
                            if script_prices:
                                prices['script_data'] = script_prices
                                break
        except Exception:
            pass
        
        # 4. Buscar padr√µes de pre√ßo no texto da p√°gina
        try:
            page_text = soup.get_text()
            # Padr√µes brasileiros de pre√ßo
            price_patterns = [
                r'R\$\s*(\d+[.,]\d{2})',
                r'(\d+[.,]\d{2})\s*reais?',
                r'valor:\s*R\$\s*(\d+[.,]\d{2})',
                r'pre√ßo:\s*R\$\s*(\d+[.,]\d{2})'
            ]
            
            found_text_prices = []
            for pattern in price_patterns:
                matches = re.findall(pattern, page_text, re.IGNORECASE)
                for match in matches:
                    try:
                        # Converter v√≠rgula para ponto
                        clean_price = match.replace(',', '.')
                        price_value = float(clean_price)
                        if 1.0 <= price_value <= 1000.0:  # Pre√ßos razo√°veis para produtos
                            found_text_prices.append(price_value)
                    except ValueError:
                        continue
            
            if found_text_prices:
                prices['text_content'] = found_text_prices
        except Exception:
            pass
        
        return prices


def main():
    """Fun√ß√£o principal do script."""
    scraper = WebScraper()
    
    try:
        scraper.run()
    except KeyboardInterrupt:
        print("\n\n‚èπÔ∏è  Scraping interrompido pelo usu√°rio")
    except Exception as e:
        print(f"\n‚ùå Erro inesperado: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()