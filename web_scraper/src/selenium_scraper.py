import json
import sys
import time
from datetime import datetime
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from database import DatabaseManager
from driver_utils import setup_driver as _setup_driver, close_driver as _close_driver
from config_loader import load_sites_config as _load_sites_config
from page_interactions import (
    handle_zipcode_modal as _handle_zipcode_modal,
    wait_for_complete_loading as _wait_for_complete_loading,
    extract_aside_content_with_monitoring as _extract_aside_content_with_monitoring,
    extract_price_via_js_selector as _extract_price_via_js_selector,
)
from report_utils import (
    display_results as _display_results,
    display_failed_summary as _display_failed_summary,
    display_database_stats as _display_database_stats,
    price_extracted_success as _price_extracted_success,
)


class SeleniumWebScraper:
    def __init__(self, config_file='data/sites.json', headless=True):
        """Inicializa o scraper com Selenium para sites com JavaScript."""
        self.config_file = config_file
        self.headless = headless
        self.sites = []
        self.driver = None
        self.db = DatabaseManager()
        
        # Configurar e inicializar o driver
        self.setup_driver()
        
        # Carregar configura√ß√£o
        self.load_config()
    
    def setup_driver(self):
        """Configura o driver do Chrome com otimiza√ß√µes."""
        try:
            self.driver = _setup_driver(self.headless)
            print("‚úÖ Driver Chrome configurado com sucesso!")
            
        except Exception as e:
            print(f"‚ùå Erro ao configurar driver Chrome: {e}")
            sys.exit(1)
    
    def load_config(self):
        """Carrega a configura√ß√£o dos sites do arquivo JSON."""
        self.sites = _load_sites_config(self.config_file)
        print(f"‚úÖ Configura√ß√£o carregada: {len(self.sites)} sites encontrados")
    
    def handle_zipcode_modal(self, zipcode=None):
        """
        Detecta e preenche o modal de CEP se aparecer.
        
        Args:
            zipcode (str): CEP a ser inserido
        """
        return _handle_zipcode_modal(self.driver, zipcode=zipcode) or False
    
    def wait_for_complete_loading(self, timeout=30, zipcode=None):
        """
        Aguarda o carregamento completo da p√°gina, incluindo JavaScript.
        
        Args:
            timeout (int): Tempo m√°ximo de espera em segundos
        """
        _wait_for_complete_loading(self.driver, timeout=timeout, zipcode=zipcode)
    
    def extract_aside_content_with_monitoring(self):
        """
        Localiza o aside com data-test='product-details-info' e monitora mudan√ßas nas tags <p>.
        
        Returns:
            dict: Dados extra√≠dos do aside com hist√≥rico de mudan√ßas
        """
        return _extract_aside_content_with_monitoring(self.driver)

    def extract_price_via_js_selector(self, price_js_expr):
        """
        Extrai o pre√ßo executando uma express√£o JavaScript (vinda do JSON) que retorna um elemento.
        A express√£o deve ser algo como: document.querySelector('...') ou equivalente.

        Retorna um dicion√°rio compat√≠vel com o formato 'aside_data' j√° usado no fluxo atual.
        """
        return _extract_price_via_js_selector(self.driver, price_js_expr)

    def scrape_site(self, site_config):
        """
        Realiza scraping aguardando JavaScript carregar e extraindo dados do aside.
        
        Args:
            site_config (dict): Configura√ß√£o do site
            
        Returns:
            dict: Dados extra√≠dos
        """
        name = site_config.get('name', 'Site Desconhecido')
        url = site_config.get('url')
        
        if not url:
            print(f"‚ö†Ô∏è  Site '{name}': URL n√£o encontrada")
            return None
            
        print(f"\nüîç Fazendo scraping de: {name}")
        print(f"   URL: {url}")
        
        try:
            # Carregar a p√°gina
            self.driver.get(url)
            
            # Aguardar carregamento completo (incluindo JavaScript), com CEP do JSON
            zipcode = site_config.get('cep') or site_config.get('zipcode')
            self.wait_for_complete_loading(zipcode=zipcode)
            
            # Debug: Capturar screenshot e verificar conte√∫do da p√°gina
            try:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                self.driver.save_screenshot(f"debug_screenshot_{timestamp}.png")
                print(f"   ‚úÖ Screenshot salvo: debug_screenshot_{timestamp}.png")
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Erro ao salvar screenshot: {e}")
            
            # Debug: Verificar se h√° JavaScript ativo
            js_check = self.driver.execute_script("return typeof jQuery !== 'undefined' || typeof $ !== 'undefined' || document.readyState;")
            if not js_check:
                print("   ‚ö†Ô∏è  JavaScript n√£o est√° ativo ou carregado corretamente.")
                raise Exception("JavaScript n√£o carregado")

            # Extrair pre√ßo via seletor definido no JSON, com fallback para l√≥gica antiga
            price_js_expr = site_config.get('price_js')
            if price_js_expr:
                aside_data = self.extract_price_via_js_selector(price_js_expr)
                # Se falhar, tenta fallback
                if not aside_data.get('aside_found'):
                    print("   ‚ö†Ô∏è  price_js n√£o retornou elemento. Tentando fallback do aside...")
                    aside_data = self.extract_aside_content_with_monitoring()
            else:
                aside_data = self.extract_aside_content_with_monitoring()
            
            # Extrair t√≠tulo da p√°gina
            title = ""
            try:
                title_element = self.driver.find_element(By.TAG_NAME, "title")
                title = title_element.get_attribute("text") or ""
            except:
                try:
                    h1_element = self.driver.find_element(By.TAG_NAME, "h1")
                    title = h1_element.text
                except:
                    title = "T√≠tulo n√£o encontrado"
            
            extracted_data = {
                'site_name': name,
                'url': url,
                'title': title.strip(),
                'scraped_at': datetime.now().isoformat(),
                'aside_data': aside_data
            }
            
            print(f"   ‚úÖ Scraping conclu√≠do!")
            return extracted_data
            
        except Exception as e:
            print(f"   ‚ùå Erro durante scraping: {e}")
            return None
    
    def save_to_database(self, site_config, result):
        """
        Salva os dados extra√≠dos no banco de dados SQLite.
        
        Args:
            site_config (dict): Configura√ß√£o do site
            result (dict): Dados extra√≠dos do scraping
        """
        try:
            # Salvar produto (usar market do JSON como site_name)
            product_id = self.db.save_product(
                name=result.get('site_name', site_config.get('name', 'Produto')),
                url=site_config.get('url'),
                site_name=site_config.get('market') or 'Desconhecido'
            )
            
            # Salvar pre√ßo
            if product_id:
                cep_value = site_config.get('cep') or site_config.get('zipcode')
                price_id = self.db.save_price(product_id, result, cep=cep_value or None)
                if price_id:
                    print(f"   üíæ Dados salvos no banco - Produto ID: {product_id}, Pre√ßo ID: {price_id}")
                else:
                    print(f"   ‚ö†Ô∏è  Produto salvo mas falha ao salvar pre√ßo")
            else:
                print(f"   ‚ùå Falha ao salvar produto no banco")
                
        except Exception as e:
            print(f"   ‚ùå Erro ao salvar no banco: {e}")
    
    def display_results(self, results):
        """Exibe os resultados do scraping no console de forma formatada."""
        _display_results(results)
    
    def display_database_stats(self):
        """Exibe estat√≠sticas do banco de dados."""
        _display_database_stats(self.db)

    def price_extracted_success(self, result):
        """
        Verifica se a extra√ß√£o do pre√ßo foi bem-sucedida.
        Retorna (success: bool, reason_if_fail: str|None)
        """
        return _price_extracted_success(result)

    def display_failed_summary(self, failed_items):
        """Exibe um resumo dos produtos cujo pre√ßo n√£o p√¥de ser extra√≠do."""
        _display_failed_summary(failed_items)

    def run(self):
        """Executa o processo completo de scraping com Selenium."""
        print("üöÄ Iniciando Web Scraper")
        print("-" * 50)
            
        # Filtrar sites habilitados
        enabled_sites = [site for site in self.sites if site.get('enabled', False)]
            
        if not enabled_sites:
            print("‚ö†Ô∏è  Nenhum site habilitado encontrado.")
            return
            
        print(f"üéØ Processando {len(enabled_sites)} site(s) habilitado(s)...")
            
        # Fazer scraping de cada site
        results = []
        failed_products = []
        for site in enabled_sites:
            result = self.scrape_site(site)
            results.append(result)

            success, reason = self.price_extracted_success(result)
            if success:
                # Salvar no banco de dados apenas quando o pre√ßo foi identificado
                self.save_to_database(site, result)
            else:
                failed_products.append({
                    'site_name': site.get('name', 'Desconhecido'),
                    'url': site.get('url'),
                    'reason': reason
                })
            
        # Exibir resultados detalhados
        self.display_results(results)
        
        # Exibir resumo dos que falharam
        self.display_failed_summary(failed_products)
        
        # Exibir estat√≠sticas do banco de dados
        self.display_database_stats()
            
        print(f"\nüéâ Scraping finalizado! Processados {len(enabled_sites)} site(s) com sucesso.")
    
    def close(self):
        _close_driver(self.driver)
        self.driver = None


def main():
    """Fun√ß√£o principal."""
    # Detectar modo baseado em argumentos ou usar modo visual por padr√£o
    import sys
    headless_mode = '--headless' in sys.argv
    
    print(f"üîß Modo: {'Headless (invis√≠vel)' if headless_mode else 'Visual (janela do navegador)'}")
    print(f"üí° Dica: Use 'python run_selenium_scraper.py --headless' para modo invis√≠vel")
    
    scraper = SeleniumWebScraper(headless=headless_mode)
    
    try:
        scraper.run()
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Scraping interrompido pelo usu√°rio.")
    except Exception as e:
        print(f"\n‚ùå Erro durante execu√ß√£o: {e}")
    finally:
        scraper.close()


if __name__ == "__main__":
    main()